Recurrent networks ("RNNs") are designed to work with sequences. Usually they are used for sentence classification (e.g. sentiment analysis) and speech recognition, but also for text generation and even image generation.
* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) - describes how RNNs can generate text, math papers and C++ code
* [10. Recurrent Neural Networks, Image Captioning, LSTM](https://www.youtube.com/watch?v=cO0a0QYmFm8&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA&index=10)
* [13. Soft attention (starting at 38:00)](https://www.youtube.com/watch?v=UFnO-ADC-k0&feature=youtu.be&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA&t=2280)
* Michael Nielsen's book stops at convolutional networks. In the [Other approaches to deep neural nets](http://neuralnetworksanddeeplearning.com/chap6.html#other_approaches_to_deep_neural_nets) section there is just a brief review of simple recurrent networks and LSTMs
* [10. Sequence Modeling: Recurrent and Recursive Nets](https://github.com/khanhnamle1994/complete-guide-to-deep-learning/blob/master/Recurrent-Neural-Networks/sequence_modeling_recurrent_recursive_neural_nets.pdf)
* [Recurrent neural networks](https://www.youtube.com/watch?v=nwcJuGuG-0s&index=8&list=PLmImxx8Char9Ig0ZHSyTqGsdhb9weEGam) from Stanford's CS224d (2016) by Richard Socher
* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

Recurrent neural networks are also implemented in every modern framework.
* [Theano: Recurrent Neural Networks with Word Embeddings](http://deeplearning.net/tutorial/rnnslu.html)
* [Theano: LSTM Networks for Sentiment Analysis](http://deeplearning.net/tutorial/lstm.html)
* [Implementing a RNN with Python, Numpy and Theano](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/)
* [Lasagne implementation of Karpathy's char-rnn](https://github.com/Lasagne/Recipes/blob/master/examples/lstm_text_generation.py)
* [Combining CNN and RNN for spoken language identification](https://yerevann.github.io/2016/06/26/combining-cnn-and-rnn-for-spoken-language-identification/) in Lasagne
* [Automatic transliteration with LSTM](http://yerevann.github.io/2016/09/09/automatic-transliteration-with-lstm/) using Lasagne
* [Tensorflow: Recurrent Neural Networks](https://www.tensorflow.org/tutorials/recurrent) for language modeling
* [Recurrent Neural Networks in Tensorflow](https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html)
* [Understanding and Implementing Deepmind's DRAW Model](http://blog.evjang.com/2016/06/understanding-and-implementing.html)
* [LSTM implementation explained](http://apaszke.github.io/lstm-explained.html)
* [Torch implementation of Karpathy's char-rnn](https://github.com/jcjohnson/torch-rnn)
